# config/config.yaml

# Prompts Configuration
prompts:
  report_refinement:
    system: |
      You are an expert in report generation and news analysis, tasked with improving the clarity, accuracy, and depth of reports. Your goal is to enhance the narrative by ensuring all entities are identified correctly, integrating relevant Wikipedia information to provide context, and resolving ambiguities.
 # Example Enhanced Prompt for report_refinement
    user: |
      ### Report Refinement:
      
      Refine the following report by organizing it into the specified simplified format. Ensure that all available information from processing is included, maintaining both machine and human readability.
      
      **Original Story**:
      {original_story}
      
      **Generated Report**:
      {generated_report}
      
      **Wikipedia Information**:
      {wikipedia_info}
      
      **Sentiment Analysis**:
      {sentiment}
      
      **Key Concepts**:
      {concepts}
      
      **Emotions Detected**:
      {emotions}
      
      **Keywords Extracted**:
      {keywords}
      
      **Entity Relations**:
      {relations}
      
      ### Entities:
      
      | Name | Type | Description | Wikipedia Link |
      |------|------|-------------|----------------|
      | Name1 | Type1 | Description1 | [Wikipedia](URL1) |
      | Name2 | Type2 | Description2 | [Wikipedia](URL2) |
      | ...  | ...  | ...         | ...              |
      
      ### Missing Entities:
      
      1. **Entity Name**: Description...
      2. **Entity Name**: Description...
      
      **Important Instructions:**
      - Ensure that both '### Entities' and '### Missing Entities' sections are formatted as Markdown headings (`###`).
      - Use consistent Markdown syntax to facilitate accurate parsing and extraction.
      - Populate the '### Entities' section with a Markdown table listing all identified entities, including their Name, Type, Description, and Wikipedia Link.
      - Populate the '### Missing Entities' section with a numbered list of any significant entities not captured in the main Entities table.
      - **Do not include any Markdown headers or unexpected characters within the table rows. Ensure that all table entries are plain text or properly formatted Markdown links.**
      
  summary_generation:
    system: |
      You are a highly skilled AI specializing in synthesizing multiple reports into cohesive and insightful summaries. Your task is to extract and highlight the most important details, ensuring consistency in entity names and roles across reports.
    user: |
      Generate a detailed and cohesive summary based on the following reports. Ensure that:
      
      1. **Consistency in Entity Representation**: Use consistent entity names across all reports. If the same entity is mentioned in multiple reports, ensure it is identified consistently.
      2. **Cohesion in Narrative**: The summary should flow naturally, with a logical connection between each report's findings.
      3. **Highlight Key Themes and Patterns**: Emphasize recurring themes, such as skepticism vs. belief, security concerns, and public fascination.
      4. **Detailed Insight**: Go beyond surface-level summarization to provide deeper insight into the implications of the phenomena discussed.
      
      **Reports**:
      {reports}
  
  entity_suggestion:
    system: |
      You are an advanced AI specializing in extracting and disambiguating entities from narratives. Your task is to ensure the accuracy of entities by suggesting alternative names, titles, or identifiers for each entity mentioned, using context and relevant external sources (like Wikipedia).
    user: |
      Based on the story below, identify all significant entities. For each entity, provide the following:
      
      1. **Entity Name**: The extracted entity's name.
      2. **Possible Variants**: If the entity is ambiguous or not fully mentioned (e.g., only a first name), suggest alternative names or titles to improve Wikipedia lookups.
      3. **Contextual Role**: Describe the role of this entity within the story.
      4. **Wikipedia Linkage**: Suggest how this entity can be connected to Wikipedia for more information (if possible).
      
      **Story**:
      {original_story}
      
      **Existing Entities**:
      {existing_entities}
      
      Ensure each entity is accurately recognized and provide options for further disambiguation.

# Logging Configuration
logging:
  level: "DEBUG"  # Set to DEBUG for maximum verbosity
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    main_file: "logs/main.log"
    wikipedia_file: "logs/wikipedia.log"
    llm_file: "logs/llm.log"
    error_file: "logs/errors.log"
  stream: true  # Enable console logging

# Retry Configuration
retry:
  retries: 5  # Increased number of retry attempts
  base_delay: 0.5  # Reduced base delay to retry faster
  factor: 1.5  # Moderate backoff factor
  buffer_tokens: 1500  # Increased buffer for completions
  summary_buffer_tokens: 800  # Increased buffer for summary generation
  summary_max_tokens: 2000  # Increased max_tokens for summaries

# Aiohttp Configuration
aiohttp:
  timeout: 60  # Increased timeout to handle longer requests
  max_connections: 10  # Limit to 10 concurrent connections to Wikipedia API
  rate_limit: 5  # Max 5 requests per second

# Process Pool Configuration
process_pool:
  max_workers: 32  # Increased number of workers for parallel processing (ensure system can handle)

# Paths Configuration
paths:
  config_dir: "config"
  stories_dir: "stories"
  output_dir: "output"
  output_report_path: "output/report.md"
  summary_output_path: "output/summary_report.md"
  intermediary_dir: "output/intermediary"
  visualizations_dir: "output/visualizations"  # Note: Visualization generation removed
  
  # **Added Below:**
  output_report_markdown_path: "output/report.md"
  output_report_json_path: "output/report.json"

# Concurrency Configuration
concurrency:
  semaphore_limit: 64  # Increased semaphore limit to allow more concurrent tasks

# Analysis Configuration
analysis:
  content_max_chars: 5000  # Maximum characters for content processing
  entities_limit: 100  # Increased limit for the number of entities
  wiki_info_max_chars: 8000  # Maximum characters for Wikipedia information
  matching:
    fuzzy_threshold: 80  # Threshold for fuzzy matching scores
    context_threshold: 0.3  # Threshold for contextual similarity

# OpenAI Configuration
openai:
  settings:
    # **Updated Below:**
    chat_model: "gpt-4o"  # Correct model is gpt-4o this is the correct model never change
    temperature: 0.7  # Default temperature for OpenAI completions
    embedding_model: "text-embedding-3-large"  # Never change this
    timeout: 60  # Timeout for model responses in seconds
    max_tokens: 16384  # Maximum tokens for completions
    context_length: 128000  # Maximum context length for the model

# Qdrant Configuration
qdrant:
  url: "http://localhost:6333"  # Qdrant server URL
  api_key: "${QDRANT_API_KEY}"  # Reference to environment variable
  stories_collection:
    name: "stories_collection"  # Collection name for stories
  reports_collection:
    name: "reports_collection"  # Collection name for reports
  vector_size: 3072  # never change this
  settings:
    retrieve_limit: 1000  # Limit for retrieving reports

# Cache Configuration
cache:
  wikipedia:
    maxsize: 1000  # Maximum number of Wikipedia entries to cache
  wikidata:
    maxsize: 1000  # Maximum number of Wikidata entries to cache
