# config/config.yaml

# Prompts Configuration
prompts:
  report_refinement:
    system: |
      You are an expert in report generation and news analysis, tasked with improving the clarity, accuracy, and depth of reports. Your goal is to enhance the narrative by ensuring all entities are identified correctly, integrating relevant Wikipedia information to provide context, and resolving ambiguities.
    user: |
      Refine the following report by:
      
      1. **Entity Verification and Enhancement**: For each entity mentioned, verify its correctness. If the entity is ambiguous, suggest more detailed names or titles.
      2. **Wikipedia Context Integration**: Use the provided Wikipedia information to add depth and clarity. Ensure the entities are linked correctly to the relevant Wikipedia articles.
      3. **Clarity and Depth**: Improve the overall clarity of the report by expanding on important sections, especially where additional context from Wikipedia can be valuable.
      4. **Missing Entities**: Identify and suggest any missing entities that are relevant to the story but are not explicitly mentioned in the generated report.
      
      **Original Story**:
      {original_story}
      
      **Generated Report**:
      {generated_report}
      
      **Wikipedia Information**:
      {wikipedia_info}
      
      **Wikipedia Sections**:
      {wikipedia_sections}

  summary_generation:
    system: |
      You are a highly skilled AI specializing in synthesizing multiple reports into cohesive and insightful summaries. Your task is to extract and highlight the most important details, ensuring consistency in entity names and roles across reports.
    user: |
      Generate a detailed and cohesive summary based on the following reports. Ensure that:
      
      1. **Consistency in Entity Representation**: Use consistent entity names across all reports. If the same entity is mentioned in multiple reports, ensure it is identified consistently.
      2. **Cohesion in Narrative**: The summary should flow naturally, with a logical connection between each report's findings.
      3. **Highlight Key Themes and Patterns**: Emphasize recurring themes, such as skepticism vs. belief, security concerns, and public fascination.
      4. **Detailed Insight**: Go beyond surface-level summarization to provide deeper insight into the implications of the phenomena discussed.
      
      **Reports**:
      {reports}

  entity_suggestion:
    system: |
      You are an advanced AI specializing in extracting and disambiguating entities from narratives. Your task is to ensure the accuracy of entities by suggesting alternative names, titles, or identifiers for each entity mentioned, using context and relevant external sources (like Wikipedia).
    user: |
      Based on the story below, identify all significant entities. For each entity, provide the following:
      
      1. **Entity Name**: The extracted entity's name.
      2. **Possible Variants**: If the entity is ambiguous or not fully mentioned (e.g., only a first name), suggest alternative names or titles to improve Wikipedia lookups.
      3. **Contextual Role**: Describe the role of this entity within the story.
      4. **Wikipedia Linkage**: Suggest how this entity can be connected to Wikipedia for more information (if possible).
      
      **Story**:
      {original_story}
      
      **Existing Entities**: 
      {existing_entities}
      
      Ensure each entity is accurately recognized and provide options for further disambiguation.

# Logging Configuration
logging:
  level: "DEBUG"  # Set to DEBUG for maximum verbosity
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    main_file: "logs/main.log"
    wikipedia_file: "logs/wikipedia.log"
    llm_file: "logs/llm.log"
    error_file: "logs/errors.log"
  stream: true  # Enable console logging

# Retry Configuration
retry:
  retries: 5  # Increased number of retry attempts
  base_delay: 0.5  # Reduced base delay to retry faster
  factor: 1.5  # Moderate backoff factor
  buffer_tokens: 1500  # Increased buffer for completions
  summary_buffer_tokens: 800  # Increased buffer for summary generation
  summary_max_tokens: 2000  # Increased max_tokens for summaries

# Aiohttp Configuration
aiohttp:
  timeout: 60  # Increased timeout to handle longer requests
  max_connections: 10  # Limit to 10 concurrent connections to Wikipedia API
  rate_limit: 5  # Max 5 requests per second

# Process Pool Configuration
process_pool:
  max_workers: 32  # Increased number of workers for parallel processing (ensure system can handle)

# Paths Configuration
paths:
  config_dir: "config"
  stories_dir: "stories"
  output_dir: "output"
  output_report_path: "output/report.md"
  summary_output_path: "output/summary_report.md"
  intermediary_dir: "output/intermediary"
  visualizations_dir: "output/visualizations"  # Note: Visualization generation removed

# Concurrency Configuration
concurrency:
  semaphore_limit: 64  # Increased semaphore limit to allow more concurrent tasks

# Analysis Configuration
analysis:
  content_max_chars: 5000  # Maximum characters for content processing
  entities_limit: 100  # Increased limit for the number of entities
  wiki_info_max_chars: 8000  # Maximum characters for Wikipedia information
  matching:
    fuzzy_threshold: 80  # Threshold for fuzzy matching scores
    context_threshold: 0.3  # Threshold for contextual similarity

# OpenAI Configuration
openai:
  settings:
    # Removed api_key to use environment variable instead
    temperature: 0.7  # Default temperature for OpenAI completions
    chat_model: "gpt-4o"  # Chat model to use
    embedding_model: "text-embedding-3-small"  # Embedding model to use
    timeout: 60  # Timeout for model responses in seconds
    max_tokens: 16384  # Maximum tokens for completions
    context_length: 128000  # Maximum context length for the model

# Qdrant Configuration
qdrant:
  url: "http://localhost:6333"  # Qdrant server URL
  api_key: "${QDRANT_API_KEY}"  # Reference to environment variable
  stories_collection: "stories_collection411"  # Collection name for stories
  reports_collection: "reports_collection411"  # Collection name for reports
  vector_size: 1536  # Size of the embedding vectors
  settings:
    retrieve_limit: 1000  # Limit for retrieving reports
    # Distance metrics can be added per collection if needed

# Cache Configuration
cache:
  wikipedia:
    maxsize: 1000  # Maximum number of Wikipedia entries to cache
  wikidata:
    maxsize: 1000  # Maximum number of Wikidata entries to cache
